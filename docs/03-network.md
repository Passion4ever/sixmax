# æ¨¡å— 3: ç¥ç»ç½‘ç»œè¯¦ç»†è®¾è®¡

> Actor-Critic ç½‘ç»œæ¶æ„ï¼Œç”¨äº PPO è®­ç»ƒ
>
> **çŠ¶æ€: âœ… å·²å®Œæˆ** | æ¨¡å—é¡ºåº: 3/3 | ä¾èµ–: encoding | å‚æ•°é‡: ~648K

---

## 1. æ¨¡å—æ¦‚è¿°

### 1.1 è®¾è®¡ç›®æ ‡

- è¾“å…¥ StateEncoder çš„ 261 ç»´ç‰¹å¾å‘é‡
- è¾“å‡ºç­–ç•¥åˆ†å¸ƒï¼ˆ6 ç»´ï¼‰å’ŒçŠ¶æ€ä»·å€¼ï¼ˆ1 ç»´ï¼‰
- æ”¯æŒ GPU é«˜æ•ˆæ¨ç†ï¼ˆA100 ä¼˜åŒ–ï¼‰
- å‚æ•°é‡æ§åˆ¶åœ¨ ~650Kï¼ˆè½»é‡çº§ï¼‰

### 1.2 åœ¨ç³»ç»Ÿä¸­çš„ä½ç½®

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¸¸æˆå¼•æ“   â”‚â”€â”€â”€â”€â–¶â”‚ çŠ¶æ€è¡¨ç¤ºå±‚  â”‚â”€â”€â”€â”€â–¶â”‚  ç¥ç»ç½‘ç»œ   â”‚
â”‚  (Engine)   â”‚     â”‚ (Encoding)  â”‚     â”‚ (Network)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚                                           â”‚
                         â–¼                                           â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Policy Head â”‚                             â”‚ Value Head  â”‚
                  â”‚  (Actor)    â”‚                             â”‚  (Critic)   â”‚
                  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                             â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                         â”‚                                           â”‚
                         â–¼                                           â–¼
                    6ç»´åŠ¨ä½œæ¦‚ç‡                                  1ç»´çŠ¶æ€ä»·å€¼
```

---

## 2. æ•´ä½“æ¶æ„

### 2.1 ç½‘ç»œç»“æ„å›¾

```
è¾“å…¥: (B, 261) çŠ¶æ€ç‰¹å¾
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Backbone Network          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Linear(261, 512) + LayerNorm + ReLUâ”‚  â† Layer 1
â”‚              â”‚                      â”‚
â”‚              â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚
â”‚  â”‚   Residual Block    â”‚            â”‚
â”‚  â”‚  Linear(512, 512)   â”‚            â”‚
â”‚  â”‚  LayerNorm + ReLU   â”‚            â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚
â”‚             â”‚ (+)  â†â”€â”€ æ®‹å·®è¿æ¥      â”‚
â”‚             â–¼                       â”‚
â”‚  Linear(512, 256) + LayerNorm + ReLUâ”‚  â† Layer 3
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â”‚ (B, 256)
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                     â”‚
       â–¼                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Policy Head â”‚       â”‚ Value Head  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤       â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Linear(256, â”‚       â”‚ Linear(256, â”‚
â”‚   128)+ReLU â”‚       â”‚   128)+ReLU â”‚
â”‚ Linear(128, â”‚       â”‚ Linear(128, â”‚
â”‚     6)      â”‚       â”‚     1)      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚                     â”‚
       â–¼                     â–¼
  (B, 6) logits         (B, 1) value
       â”‚
       â–¼
  masked_softmax
       â”‚
       â–¼
  (B, 6) action_probs
```

### 2.2 ç»´åº¦æµè½¬

| å±‚ | è¾“å…¥ç»´åº¦ | è¾“å‡ºç»´åº¦ | è¯´æ˜ |
|----|----------|----------|------|
| è¾“å…¥ | (B, 261) | - | StateEncoder è¾“å‡º |
| Backbone Layer 1 | (B, 261) | (B, 512) | å‡ç»´ |
| Residual Block | (B, 512) | (B, 512) | æ®‹å·®è¿æ¥ |
| Backbone Layer 3 | (B, 512) | (B, 256) | é™ç»´ |
| Policy Head | (B, 256) | (B, 6) | åŠ¨ä½œ logits |
| Value Head | (B, 256) | (B, 1) | çŠ¶æ€ä»·å€¼ |

---

## 3. æ¨¡å—è¯¦ç»†è®¾è®¡

### 3.1 Backbone Network

**è®¾è®¡åŸåˆ™:**
- ä½¿ç”¨ LayerNorm è€Œé BatchNormï¼ˆRL è®­ç»ƒæ›´ç¨³å®šï¼‰
- æ®‹å·®è¿æ¥é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
- æ­£äº¤åˆå§‹åŒ–ï¼ˆPPO æœ€ä½³å®è·µï¼‰

```python
class Backbone(nn.Module):
    """
    å…±äº«ä¸»å¹²ç½‘ç»œã€‚

    ç»“æ„: 261 â†’ 512 â†’ 512(æ®‹å·®) â†’ 256
    """

    def __init__(self, input_dim: int = 261, hidden_dim: int = 512, output_dim: int = 256):
        super().__init__()

        # Layer 1: å‡ç»´
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.ln1 = nn.LayerNorm(hidden_dim)

        # Residual Block
        self.res_fc = nn.Linear(hidden_dim, hidden_dim)
        self.res_ln = nn.LayerNorm(hidden_dim)

        # Layer 3: é™ç»´
        self.fc2 = nn.Linear(hidden_dim, output_dim)
        self.ln2 = nn.LayerNorm(output_dim)

        # æ­£äº¤åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        for module in [self.fc1, self.res_fc, self.fc2]:
            nn.init.orthogonal_(module.weight, gain=np.sqrt(2))
            nn.init.zeros_(module.bias)

    def forward(self, x: Tensor) -> Tensor:
        # Layer 1
        x = F.relu(self.ln1(self.fc1(x)))

        # Residual Block
        residual = x
        x = F.relu(self.res_ln(self.res_fc(x)))
        x = x + residual  # æ®‹å·®è¿æ¥

        # Layer 3
        x = F.relu(self.ln2(self.fc2(x)))

        return x  # (B, 256)
```

**å‚æ•°é‡:**
- fc1: 261 Ã— 512 + 512 = 134,144
- ln1: 512 Ã— 2 = 1,024
- res_fc: 512 Ã— 512 + 512 = 262,656
- res_ln: 512 Ã— 2 = 1,024
- fc2: 512 Ã— 256 + 256 = 131,328
- ln2: 256 Ã— 2 = 512
- **æ€»è®¡: ~531K**

### 3.2 Policy Head (Actor)

**èŒè´£:** è¾“å‡ºåŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ

```python
class PolicyHead(nn.Module):
    """
    ç­–ç•¥å¤´ (Actor)ã€‚

    è¾“å‡º 6 ç»´ logitsï¼Œç»è¿‡ masked softmax å¾—åˆ°åŠ¨ä½œæ¦‚ç‡ã€‚
    """

    def __init__(self, input_dim: int = 256, hidden_dim: int = 128, num_actions: int = 6):
        super().__init__()

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, num_actions)

        # ç­–ç•¥å¤´ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–ï¼ˆè¾“å‡ºæ¥è¿‘å‡åŒ€åˆ†å¸ƒï¼‰
        self._init_weights()

    def _init_weights(self):
        nn.init.orthogonal_(self.fc1.weight, gain=np.sqrt(2))
        nn.init.zeros_(self.fc1.bias)
        nn.init.orthogonal_(self.fc2.weight, gain=0.01)  # å°å¢ç›Š
        nn.init.zeros_(self.fc2.bias)

    def forward(self, x: Tensor, legal_mask: Tensor) -> tuple[Tensor, Tensor]:
        """
        Args:
            x: (B, 256) backbone è¾“å‡º
            legal_mask: (B, 6) åˆæ³•åŠ¨ä½œæ©ç  (True=åˆæ³•)

        Returns:
            action_probs: (B, 6) åŠ¨ä½œæ¦‚ç‡åˆ†å¸ƒ
            logits: (B, 6) åŸå§‹ logits
        """
        x = F.relu(self.fc1(x))
        logits = self.fc2(x)  # (B, 6)

        # å±è”½éæ³•åŠ¨ä½œ
        masked_logits = logits.masked_fill(~legal_mask, float('-inf'))
        action_probs = F.softmax(masked_logits, dim=-1)

        return action_probs, logits
```

**å‚æ•°é‡:**
- fc1: 256 Ã— 128 + 128 = 32,896
- fc2: 128 Ã— 6 + 6 = 774
- **æ€»è®¡: ~33K**

### 3.3 Value Head (Critic)

**èŒè´£:** ä¼°è®¡çŠ¶æ€ä»·å€¼

```python
class ValueHead(nn.Module):
    """
    ä»·å€¼å¤´ (Critic)ã€‚

    è¾“å‡ºæ ‡é‡çŠ¶æ€ä»·å€¼ï¼ŒèŒƒå›´çº¦ [-100, +100] BBã€‚
    """

    def __init__(self, input_dim: int = 256, hidden_dim: int = 128):
        super().__init__()

        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, 1)

        self._init_weights()

    def _init_weights(self):
        nn.init.orthogonal_(self.fc1.weight, gain=np.sqrt(2))
        nn.init.zeros_(self.fc1.bias)
        nn.init.orthogonal_(self.fc2.weight, gain=1.0)
        nn.init.zeros_(self.fc2.bias)

    def forward(self, x: Tensor) -> Tensor:
        """
        Args:
            x: (B, 256) backbone è¾“å‡º

        Returns:
            value: (B, 1) çŠ¶æ€ä»·å€¼
        """
        x = F.relu(self.fc1(x))
        value = self.fc2(x)  # (B, 1)

        return value
```

**å‚æ•°é‡:**
- fc1: 256 Ã— 128 + 128 = 32,896
- fc2: 128 Ã— 1 + 1 = 129
- **æ€»è®¡: ~33K**

---

## 4. å®Œæ•´ç½‘ç»œå®ç°

### 4.1 PolicyValueNetwork

```python
class PolicyValueNetwork(nn.Module):
    """
    Actor-Critic ç½‘ç»œã€‚

    ç»“åˆ StateEncoderã€Backboneã€PolicyHeadã€ValueHeadã€‚

    è¾“å…¥: æ¸¸æˆçŠ¶æ€ batch
    è¾“å‡º: åŠ¨ä½œæ¦‚ç‡ã€çŠ¶æ€ä»·å€¼
    """

    def __init__(
        self,
        state_encoder: StateEncoder | None = None,
        backbone_hidden: int = 512,
        backbone_output: int = 256,
        head_hidden: int = 128,
        num_actions: int = 6,
    ):
        super().__init__()

        # çŠ¶æ€ç¼–ç å™¨ï¼ˆå¯é€‰ï¼Œä¹Ÿå¯ä»¥å¤–éƒ¨ä¼ å…¥å·²ç¼–ç ç‰¹å¾ï¼‰
        self.state_encoder = state_encoder or StateEncoder()
        input_dim = self.state_encoder.get_output_dim()  # 261

        # ä¸»å¹²ç½‘ç»œ
        self.backbone = Backbone(input_dim, backbone_hidden, backbone_output)

        # è¾“å‡ºå¤´
        self.policy_head = PolicyHead(backbone_output, head_hidden, num_actions)
        self.value_head = ValueHead(backbone_output, head_hidden)

    def forward(
        self,
        batch: dict[str, Tensor],
        legal_mask: Tensor,
    ) -> tuple[Tensor, Tensor, Tensor]:
        """
        å‰å‘ä¼ æ’­ã€‚

        Args:
            batch: æ¸¸æˆçŠ¶æ€ batch (æ¥è‡ª StateBatchBuilder)
            legal_mask: (B, 6) åˆæ³•åŠ¨ä½œæ©ç 

        Returns:
            action_probs: (B, 6) åŠ¨ä½œæ¦‚ç‡
            value: (B, 1) çŠ¶æ€ä»·å€¼
            logits: (B, 6) åŸå§‹ logits (ç”¨äºè®¡ç®—æŸå¤±)
        """
        # 1. çŠ¶æ€ç¼–ç 
        state_features = self.state_encoder(batch)  # (B, 261)

        # 2. Backbone
        backbone_out = self.backbone(state_features)  # (B, 256)

        # 3. è¾“å‡ºå¤´
        action_probs, logits = self.policy_head(backbone_out, legal_mask)
        value = self.value_head(backbone_out)

        return action_probs, value, logits

    def get_action(
        self,
        batch: dict[str, Tensor],
        legal_mask: Tensor,
        deterministic: bool = False,
    ) -> tuple[Tensor, Tensor, Tensor]:
        """
        é‡‡æ ·åŠ¨ä½œï¼ˆç”¨äºæ¨ç†/æ”¶é›†æ•°æ®ï¼‰ã€‚

        Args:
            batch: æ¸¸æˆçŠ¶æ€ batch
            legal_mask: (B, 6) åˆæ³•åŠ¨ä½œæ©ç 
            deterministic: æ˜¯å¦é€‰æ‹©æœ€é«˜æ¦‚ç‡åŠ¨ä½œ

        Returns:
            action: (B,) é‡‡æ ·çš„åŠ¨ä½œ
            log_prob: (B,) åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: (B, 1) çŠ¶æ€ä»·å€¼
        """
        action_probs, value, _ = self.forward(batch, legal_mask)

        if deterministic:
            action = action_probs.argmax(dim=-1)
        else:
            dist = torch.distributions.Categorical(action_probs)
            action = dist.sample()

        # è®¡ç®— log probability
        log_prob = torch.log(action_probs.gather(1, action.unsqueeze(-1)) + 1e-8).squeeze(-1)

        return action, log_prob, value

    def evaluate_actions(
        self,
        batch: dict[str, Tensor],
        legal_mask: Tensor,
        actions: Tensor,
    ) -> tuple[Tensor, Tensor, Tensor]:
        """
        è¯„ä¼°ç»™å®šåŠ¨ä½œï¼ˆç”¨äº PPO æ›´æ–°ï¼‰ã€‚

        Args:
            batch: æ¸¸æˆçŠ¶æ€ batch
            legal_mask: (B, 6) åˆæ³•åŠ¨ä½œæ©ç 
            actions: (B,) å®é™…æ‰§è¡Œçš„åŠ¨ä½œ

        Returns:
            log_prob: (B,) åŠ¨ä½œçš„å¯¹æ•°æ¦‚ç‡
            value: (B, 1) çŠ¶æ€ä»·å€¼
            entropy: (B,) ç­–ç•¥ç†µ
        """
        action_probs, value, _ = self.forward(batch, legal_mask)

        dist = torch.distributions.Categorical(action_probs)
        log_prob = dist.log_prob(actions)
        entropy = dist.entropy()

        return log_prob, value, entropy
```

### 4.2 è¾…åŠ©å‡½æ•°

```python
def count_parameters(model: nn.Module) -> int:
    """ç»Ÿè®¡æ¨¡å‹å‚æ•°é‡ã€‚"""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def create_policy_value_network(device: str = "cpu") -> PolicyValueNetwork:
    """åˆ›å»ºé»˜è®¤é…ç½®çš„ç½‘ç»œã€‚"""
    network = PolicyValueNetwork()
    network = network.to(device)
    return network
```

---

## 5. å‚æ•°ç»Ÿè®¡

### 5.1 å„æ¨¡å—å‚æ•°é‡

| æ¨¡å— | å‚æ•°é‡ | å æ¯” |
|------|--------|------|
| StateEncoder (å·²å®ç°) | ~51K | 8% |
| Backbone | ~531K | 82% |
| PolicyHead | ~33K | 5% |
| ValueHead | ~33K | 5% |
| **æ€»è®¡** | **~648K** | 100% |

### 5.2 ä¸å…¶ä»–æ¨¡å‹å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | è¯´æ˜ |
|------|--------|------|
| æœ¬é¡¹ç›® | 0.65M | è½»é‡çº§ |
| AlphaGo Policy | 13M | 20x |
| GPT-2 Small | 124M | 190x |
| BERT-base | 110M | 169x |

---

## 6. æ€§èƒ½é¢„ä¼°

### 6.1 A100 GPU æ€§èƒ½

| æŒ‡æ ‡ | é¢„ä¼°å€¼ | è¯´æ˜ |
|------|--------|------|
| å•æ¬¡å‰å‘ | < 1ms | batch_size=1 |
| Batch 4096 | ~10ms | æ¨è batch |
| Batch 16384 | ~40ms | æœ€å¤§ batch |
| æ˜¾å­˜å ç”¨ | ~50MB | æ¨¡å‹æƒé‡ |

### 6.2 æ¨èé…ç½®

```python
# è®­ç»ƒé…ç½®
train_config = {
    "batch_size": 4096,      # æ¨è batch å¤§å°
    "learning_rate": 3e-4,   # PPO æ ‡å‡†å­¦ä¹ ç‡
    "max_grad_norm": 0.5,    # æ¢¯åº¦è£å‰ª
    "num_epochs": 4,         # PPO æ›´æ–°è½®æ•°
}

# æ¨ç†é…ç½®
infer_config = {
    "batch_size": 1024,      # æ¨ç† batch
    "deterministic": False,  # æ¢ç´¢æ¨¡å¼
}
```

---

## 7. ä¸å…¶ä»–æ¨¡å—çš„æ¥å£

### 7.1 è¾“å…¥æ¥å£

```python
# ä» StateBatchBuilder è·å–è¾“å…¥
from sixmax.encoding import StateBatchBuilder

builder = StateBatchBuilder()
batch = builder.build_batch(games, player_ids)

# batch åŒ…å«:
# - hole_ranks: (B, 2)
# - hole_suits: (B, 2)
# - board_ranks: (B, 5)
# - board_suits: (B, 5)
# - self_info: (B, 14)
# - opponent_info: (B, 15)
# - actions: (B, 24, 17)
# - action_mask: (B, 24)
```

### 7.2 è¾“å‡ºæ¥å£

```python
# ç½‘ç»œè¾“å‡º
action_probs, value, logits = network(batch, legal_mask)

# action_probs: (B, 6) - ç”¨äºé‡‡æ ·åŠ¨ä½œ
# value: (B, 1) - ç”¨äº PPO ä¼˜åŠ¿ä¼°è®¡
# logits: (B, 6) - ç”¨äºè®¡ç®—æŸå¤±
```

### 7.3 ä¸æ¸¸æˆå¼•æ“çš„é›†æˆ

```python
# å®Œæ•´æ¨ç†æµç¨‹
game = PokerGame()
game.reset_hand()

# 1. è·å–çŠ¶æ€
state_dict = game.get_state_for_player(player_id=0)

# 2. æ„å»º batch
batch = builder.build_from_dict(state_dict)
batch = {k: v.unsqueeze(0).to(device) for k, v in batch.items()}

# 3. è·å–åˆæ³•åŠ¨ä½œ
legal_actions = game.get_legal_actions()
legal_mask = torch.tensor([legal_actions], device=device)

# 4. ç½‘ç»œæ¨ç†
with torch.no_grad():
    action, log_prob, value = network.get_action(batch, legal_mask)

# 5. æ‰§è¡ŒåŠ¨ä½œ
action_type = ActionType(action.item())
game.step(action_type)
```

---

## 8. æ–‡ä»¶ç»“æ„

```
src/sixmax/network/
â”œâ”€â”€ __init__.py          # å¯¼å‡ºå…¬å…±æ¥å£
â”œâ”€â”€ backbone.py          # Backbone ç½‘ç»œ
â”œâ”€â”€ heads.py             # PolicyHead, ValueHead
â”œâ”€â”€ policy_value.py      # PolicyValueNetwork å®Œæ•´ç½‘ç»œ
â””â”€â”€ utils.py             # è¾…åŠ©å‡½æ•°

tests/
â””â”€â”€ test_network.py      # ç½‘ç»œæµ‹è¯•
```

---

## 9. å®ç°æ¸…å•

| ç»„ä»¶ | ä¼˜å…ˆçº§ | çŠ¶æ€ |
|------|--------|------|
| Backbone | P0 | âœ… å·²å®Œæˆ |
| PolicyHead | P0 | âœ… å·²å®Œæˆ |
| ValueHead | P0 | âœ… å·²å®Œæˆ |
| PolicyValueNetwork | P0 | âœ… å·²å®Œæˆ |
| å•å…ƒæµ‹è¯• | P0 | âœ… å·²å®Œæˆ (36 ä¸ª) |
| GPU ä¼˜åŒ– | P1 | ğŸ”² åç»­ |
| torch.compile | P1 | ğŸ”² åç»­ |

---

## é™„å½•

### A. åˆå§‹åŒ–ç­–ç•¥

| æ¨¡å— | åˆå§‹åŒ–æ–¹æ³• | gain |
|------|-----------|------|
| Backbone çº¿æ€§å±‚ | æ­£äº¤åˆå§‹åŒ– | âˆš2 |
| PolicyHead fc1 | æ­£äº¤åˆå§‹åŒ– | âˆš2 |
| PolicyHead fc2 | æ­£äº¤åˆå§‹åŒ– | 0.01 |
| ValueHead fc1 | æ­£äº¤åˆå§‹åŒ– | âˆš2 |
| ValueHead fc2 | æ­£äº¤åˆå§‹åŒ– | 1.0 |
| æ‰€æœ‰ bias | é›¶åˆå§‹åŒ– | - |

### B. PPO ç›¸å…³æ¥å£

```python
# PPO è®­ç»ƒéœ€è¦çš„æ¥å£
class PolicyValueNetwork:
    def forward(batch, legal_mask) -> (probs, value, logits)
    def get_action(batch, legal_mask, deterministic) -> (action, log_prob, value)
    def evaluate_actions(batch, legal_mask, actions) -> (log_prob, value, entropy)
```

### C. å‚è€ƒèµ„æ–™

- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [Stable-Baselines3 PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)
- [CleanRL PPO](https://docs.cleanrl.dev/rl-algorithms/ppo/)
