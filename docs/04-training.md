# æ¨¡å— 4: PPO è®­ç»ƒç³»ç»Ÿè¯¦ç»†è®¾è®¡

> è‡ªåšå¼ˆ + PPO å¼ºåŒ–å­¦ä¹ è®­ç»ƒæ¡†æ¶
>
> **çŠ¶æ€: ğŸ”² å¾…å®ç°** | æ¨¡å—é¡ºåº: 4/5 | ä¾èµ–: engine, encoding, network

---

## ç›®å½•

1. [æ¨¡å—æ¦‚è¿°](#1-æ¨¡å—æ¦‚è¿°)
2. [è®­ç»ƒæ¶æ„](#2-è®­ç»ƒæ¶æ„)
3. [è‡ªåšå¼ˆç³»ç»Ÿ](#3-è‡ªåšå¼ˆç³»ç»Ÿ)
4. [PPO ç®—æ³•](#4-ppo-ç®—æ³•)
5. [å¥–åŠ±è®¾è®¡](#5-å¥–åŠ±è®¾è®¡)
6. [æ¨¡å—ç»“æ„](#6-æ¨¡å—ç»“æ„)
7. [æ ¸å¿ƒæ¥å£](#7-æ ¸å¿ƒæ¥å£)
8. [è¿è¡Œç¯å¢ƒ](#8-è¿è¡Œç¯å¢ƒ)
9. [å®ç°æ¸…å•](#9-å®ç°æ¸…å•)

---

## 1. æ¨¡å—æ¦‚è¿°

### 1.1 è®¾è®¡ç›®æ ‡

- å®ç° PPO è‡ªåšå¼ˆè®­ç»ƒå¾ªç¯
- æ”¯æŒå• GPU åŒæ­¥è®­ç»ƒï¼ˆPhase 1ï¼‰å’ŒåŒ GPU å¼‚æ­¥è®­ç»ƒï¼ˆPhase 2ï¼‰
- é›†æˆ wandb å®éªŒè¿½è¸ª
- æ”¯æŒ SLURM é›†ç¾¤è°ƒåº¦

### 1.2 åœ¨ç³»ç»Ÿä¸­çš„ä½ç½®

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  æ¸¸æˆå¼•æ“   â”‚â”€â”€â”€â”€â–¶â”‚ çŠ¶æ€è¡¨ç¤ºå±‚  â”‚â”€â”€â”€â”€â–¶â”‚  ç¥ç»ç½‘ç»œ   â”‚
â”‚  (Engine)   â”‚     â”‚ (Encoding)  â”‚     â”‚ (Network)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    è®­ç»ƒç³»ç»Ÿ (PPO)   â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
         â”‚  â€¢ è‡ªåšå¼ˆæ•°æ®æ”¶é›†   â”‚
         â”‚  â€¢ PPO ç­–ç•¥æ›´æ–°     â”‚
         â”‚  â€¢ ç»éªŒç¼“å†²åŒº       â”‚
         â”‚  â€¢ wandb æ—¥å¿—       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 æ¸è¿›å¼å¼€å‘è·¯çº¿

| é˜¶æ®µ | ç›®æ ‡ | ç‰¹æ€§ |
|------|------|------|
| Phase 1 | éªŒè¯ç®—æ³• | åŒæ­¥è‡ªåšå¼ˆã€ç¨€ç–å¥–åŠ±ã€å• GPU |
| Phase 2 | æ€§èƒ½ä¼˜åŒ– | å¼‚æ­¥æ¶æ„ã€å†å²å¯¹æ‰‹æ± ã€åŒ GPU |
| Phase 3 | é«˜çº§ç‰¹æ€§ | TD(Î») å¥–åŠ±ã€å¯¹æ‰‹å»ºæ¨¡ã€EV ä¼°è®¡ |

---

## 2. è®­ç»ƒæ¶æ„

### 2.1 Phase 1: åŒæ­¥è‡ªåšå¼ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å• GPU åŒæ­¥è®­ç»ƒ                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚ æ”¶é›†æ•°æ® â”‚â”€â”€â”€â–¶â”‚ è®¡ç®—ä¼˜åŠ¿ â”‚â”€â”€â”€â–¶â”‚ PPOæ›´æ–°  â”‚        â”‚
â”‚  â”‚(16384å±€) â”‚    â”‚  (GAE)   â”‚    â”‚ (4epochs)â”‚        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
â”‚       â”‚                               â”‚               â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ å¾ªç¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç‰¹ç‚¹**:
- å•è¿›ç¨‹ï¼Œæ˜“è°ƒè¯•
- æ”¶é›†å’Œè®­ç»ƒäº¤æ›¿è¿›è¡Œ
- GPU åˆ©ç”¨ç‡çº¦ 50%

### 2.2 Phase 2: å¼‚æ­¥ Actor-Learner

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   åŒ GPU å¼‚æ­¥è®­ç»ƒ                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                        â”‚
â”‚  GPU 1 (æ¨ç†)              GPU 2 (è®­ç»ƒ)               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Actor 1    â”‚          â”‚              â”‚           â”‚
â”‚  â”‚   Actor 2    â”‚â”€â”€Queueâ”€â”€â–¶â”‚   Learner    â”‚           â”‚
â”‚  â”‚   Actor N    â”‚          â”‚              â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚        â”‚                          â”‚                   â”‚
â”‚        â””â”€â”€â”€â”€â”€â”€ å®šæœŸåŒæ­¥æƒé‡ â”€â”€â”€â”€â”€â”€â”˜                   â”‚
â”‚                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ç‰¹ç‚¹**:
- æ”¶é›†å’Œè®­ç»ƒå¹¶è¡Œ
- GPU åˆ©ç”¨ç‡ > 90%
- éœ€è¦å¤„ç†æƒé‡åŒæ­¥

---

## 3. è‡ªåšå¼ˆç³»ç»Ÿ

### 3.1 Phase 1: çº¯è‡ªåšå¼ˆ

```python
# 6 ä¸ªç©å®¶å…¨éƒ¨ä½¿ç”¨å½“å‰ç½‘ç»œ
players = [current_network] * 6
```

**ä¼˜ç‚¹**: ç®€å•
**ç¼ºç‚¹**: å¯èƒ½é™·å…¥å¾ªç¯ç­–ç•¥

### 3.2 Phase 2: æ··åˆå¯¹æ‰‹æ± 

```python
# 70% ä½¿ç”¨å½“å‰ç½‘ç»œï¼Œ30% ä½¿ç”¨å†å²ç‰ˆæœ¬
def select_opponents():
    opponents = []
    for _ in range(5):  # 5 ä¸ªå¯¹æ‰‹
        if random.random() < 0.7:
            opponents.append(current_network)
        else:
            opponents.append(random.choice(history_pool))
    return opponents
```

**å¯¹æ‰‹æ± ç®¡ç†**:
- æ¯ 10000 æ‰‹ä¿å­˜ä¸€ä¸ªç‰ˆæœ¬
- ä¿ç•™æœ€è¿‘ 10 ä¸ªç‰ˆæœ¬
- å¯é€‰: åŸºäº Elo è¯„åˆ†ç­›é€‰

### 3.3 æ•°æ®æ”¶é›†æµç¨‹

```python
def collect_rollout(games, network, n_hands=2000):
    """
    æ”¶é›†è‡ªåšå¼ˆæ•°æ®ã€‚

    Args:
        games: å¹¶è¡Œæ¸¸æˆå®ä¾‹ (16384 ä¸ª)
        network: å½“å‰ç­–ç•¥ç½‘ç»œ
        n_hands: æ”¶é›†æ‰‹æ•°

    Returns:
        buffer: ç»éªŒç¼“å†²åŒº
    """
    buffer = RolloutBuffer()
    hands_played = 0

    while hands_played < n_hands:
        # 1. è·å–æ‰€æœ‰æ¸¸æˆçš„å½“å‰çŠ¶æ€
        states = [g.get_state() for g in games]
        batch = build_batch(states)
        legal_masks = get_legal_masks(games)

        # 2. æ‰¹é‡æ¨ç†é€‰åŠ¨ä½œ
        with torch.no_grad():
            actions, log_probs, values = network.get_action(batch, legal_masks)

        # 3. æ‰§è¡ŒåŠ¨ä½œ
        for i, game in enumerate(games):
            reward = game.step(actions[i])
            done = game.is_hand_over()

            buffer.add(
                state=states[i],
                action=actions[i],
                reward=reward,
                value=values[i],
                log_prob=log_probs[i],
                done=done
            )

            if done:
                game.reset_hand()
                hands_played += 1

    return buffer
```

---

## 4. PPO ç®—æ³•

### 4.1 æŸå¤±å‡½æ•°

```python
def compute_ppo_loss(batch, network, clip_epsilon=0.2):
    """
    è®¡ç®— PPO æŸå¤±ã€‚

    L = L_policy + c1 * L_value - c2 * L_entropy
    """
    # é‡æ–°è¯„ä¼°åŠ¨ä½œ
    new_log_probs, new_values, entropy = network.evaluate_actions(
        batch.states, batch.legal_masks, batch.actions
    )

    # ç­–ç•¥æŸå¤± (PPO-Clip)
    ratio = torch.exp(new_log_probs - batch.old_log_probs)
    surr1 = ratio * batch.advantages
    surr2 = torch.clamp(ratio, 1 - clip_epsilon, 1 + clip_epsilon) * batch.advantages
    policy_loss = -torch.min(surr1, surr2).mean()

    # ä»·å€¼æŸå¤±
    value_loss = F.mse_loss(new_values.squeeze(), batch.returns)

    # ç†µæ­£åˆ™åŒ–
    entropy_loss = -entropy.mean()

    # æ€»æŸå¤±
    total_loss = policy_loss + 0.5 * value_loss + 0.01 * entropy_loss

    return total_loss, {
        'policy_loss': policy_loss.item(),
        'value_loss': value_loss.item(),
        'entropy': entropy.mean().item(),
    }
```

### 4.2 GAE ä¼˜åŠ¿ä¼°è®¡

```python
def compute_gae(rewards, values, dones, gamma=0.99, gae_lambda=0.95):
    """
    è®¡ç®— Generalized Advantage Estimationã€‚

    A_t = Î´_t + (Î³Î»)Î´_{t+1} + (Î³Î»)Â²Î´_{t+2} + ...
    Î´_t = r_t + Î³V(s_{t+1}) - V(s_t)
    """
    advantages = torch.zeros_like(rewards)
    last_gae = 0

    for t in reversed(range(len(rewards))):
        if t == len(rewards) - 1:
            next_value = 0
        else:
            next_value = values[t + 1]

        delta = rewards[t] + gamma * next_value * (1 - dones[t]) - values[t]
        advantages[t] = last_gae = delta + gamma * gae_lambda * (1 - dones[t]) * last_gae

    returns = advantages + values
    return advantages, returns
```

### 4.3 è¶…å‚æ•°é…ç½®

| å‚æ•° | åˆå§‹å€¼ | è¯´æ˜ |
|------|--------|------|
| learning_rate | 3e-4 | Adam å­¦ä¹ ç‡ |
| clip_epsilon | 0.2 | PPO è£å‰ªèŒƒå›´ |
| value_coef | 0.5 | ä»·å€¼æŸå¤±æƒé‡ |
| entropy_coef | 0.01 | ç†µæ­£åˆ™åŒ–ç³»æ•° |
| gamma | 0.99 | æŠ˜æ‰£å› å­ |
| gae_lambda | 0.95 | GAE å‚æ•° |
| n_epochs | 4 | æ¯æ‰¹æ•°æ®è®­ç»ƒè½®æ•° |
| batch_size | 4096 | å°æ‰¹é‡å¤§å° |
| max_grad_norm | 0.5 | æ¢¯åº¦è£å‰ª |
| n_games | 16384 | å¹¶è¡Œæ¸¸æˆæ•° |
| n_hands_per_update | 2000 | æ¯æ¬¡æ›´æ–°çš„æ‰‹æ•° |

---

## 5. å¥–åŠ±è®¾è®¡

### 5.1 Phase 1: ç¨€ç–å¥–åŠ±

```python
def compute_reward(game, player_id):
    """
    æ‰‹ç‰Œç»“æŸæ—¶çš„ç­¹ç å˜åŒ– (BB å•ä½)ã€‚
    """
    if not game.is_hand_over():
        return 0.0

    initial_stack = 100.0  # 100 BB
    final_stack = game.get_player_stack(player_id)
    return final_stack - initial_stack
```

**ç‰¹ç‚¹**:
- ç®€å•ï¼Œæ— äººä¸ºåå·®
- æ–¹å·®å¤§ï¼ˆå¥½å†³ç­–å¯èƒ½å› è¿æ°”å¾—è´Ÿå¥–åŠ±ï¼‰
- ä¾èµ– GAE å’Œå¤§æ ·æœ¬é‡

### 5.2 Phase 2: TD Bootstrap

```python
def compute_td_reward(game, player_id, value_network):
    """
    ä½¿ç”¨ä»·å€¼ç½‘ç»œé™ä½æ–¹å·®ã€‚
    """
    if game.is_hand_over():
        return game.get_player_stack(player_id) - 100.0
    else:
        # ç”¨ V(s') ä¼°è®¡æœªæ¥æ”¶ç›Š
        next_state = game.get_state_for_player(player_id)
        with torch.no_grad():
            next_value = value_network.get_value(next_state)
        return gamma * next_value
```

### 5.3 Phase 3: EV ä¼°è®¡

é…åˆå¯¹æ‰‹å»ºæ¨¡ï¼Œæ›´ç²¾ç¡®ä¼°è®¡åŠ¨ä½œçš„æœŸæœ›å€¼ã€‚

---

## 6. æ¨¡å—ç»“æ„

```
src/sixmax/training/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ buffer.py           # ç»éªŒç¼“å†²åŒº
â”œâ”€â”€ ppo.py              # PPO ç®—æ³•æ ¸å¿ƒ
â”œâ”€â”€ rollout.py          # è‡ªåšå¼ˆæ•°æ®æ”¶é›†
â”œâ”€â”€ trainer.py          # è®­ç»ƒä¸»å¾ªç¯
â”œâ”€â”€ config.py           # è¶…å‚æ•°é…ç½®
â””â”€â”€ utils.py            # wandb æ—¥å¿—ã€æ£€æŸ¥ç‚¹

scripts/
â”œâ”€â”€ train.py            # è®­ç»ƒå…¥å£
â”œâ”€â”€ slurm_train.sh      # SLURM æäº¤è„šæœ¬
â””â”€â”€ eval.py             # æ¨¡å‹è¯„ä¼°
```

### 6.1 å„æ¨¡å—èŒè´£

| æ¨¡å— | èŒè´£ |
|------|------|
| buffer.py | å­˜å‚¨è½¨è¿¹ (s,a,r,v,log_p)ï¼Œè®¡ç®— GAE/Returns |
| ppo.py | PPO æŸå¤±è®¡ç®—ï¼Œç½‘ç»œæ›´æ–° |
| rollout.py | æ‰¹é‡è¿è¡Œæ¸¸æˆï¼Œæ”¶é›†ç»éªŒ |
| trainer.py | ä¸»å¾ªç¯ï¼šæ”¶é›† â†’ GAE â†’ æ›´æ–° â†’ æ—¥å¿— |
| config.py | æ‰€æœ‰è¶…å‚æ•°é›†ä¸­ç®¡ç† |
| utils.py | wandb åˆå§‹åŒ–ã€æ¨¡å‹ä¿å­˜/åŠ è½½ |

---

## 7. æ ¸å¿ƒæ¥å£

### 7.1 RolloutBuffer

```python
class RolloutBuffer:
    """ç»éªŒç¼“å†²åŒºã€‚"""

    def __init__(self, gamma: float = 0.99, gae_lambda: float = 0.95):
        self.gamma = gamma
        self.gae_lambda = gae_lambda
        self.reset()

    def reset(self):
        """æ¸…ç©ºç¼“å†²åŒºã€‚"""
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []

    def add(self, state, action, reward, value, log_prob, done):
        """æ·»åŠ ä¸€æ¡ç»éªŒã€‚"""
        ...

    def compute_returns_and_advantages(self, last_value: float):
        """è®¡ç®— GAE ä¼˜åŠ¿å’Œå›æŠ¥ã€‚"""
        ...

    def get_batches(self, batch_size: int):
        """ç”Ÿæˆè®­ç»ƒå°æ‰¹é‡ã€‚"""
        ...
```

### 7.2 PPOTrainer

```python
class PPOTrainer:
    """PPO è®­ç»ƒå™¨ã€‚"""

    def __init__(
        self,
        network: PolicyValueNetwork,
        config: TrainingConfig,
    ):
        self.network = network
        self.config = config
        self.optimizer = torch.optim.Adam(
            network.parameters(),
            lr=config.learning_rate
        )

    def collect_rollouts(self, games: list, n_hands: int) -> RolloutBuffer:
        """æ”¶é›†è‡ªåšå¼ˆæ•°æ®ã€‚"""
        ...

    def update(self, buffer: RolloutBuffer) -> dict:
        """æ‰§è¡Œ PPO æ›´æ–°ï¼Œè¿”å›æ—¥å¿—æŒ‡æ ‡ã€‚"""
        ...

    def train(self, total_hands: int):
        """è®­ç»ƒä¸»å¾ªç¯ã€‚"""
        ...

    def save_checkpoint(self, path: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹ã€‚"""
        ...

    def load_checkpoint(self, path: str):
        """åŠ è½½æ£€æŸ¥ç‚¹ã€‚"""
        ...
```

### 7.3 TrainingConfig

```python
@dataclass
class TrainingConfig:
    """è®­ç»ƒè¶…å‚æ•°é…ç½®ã€‚"""

    # PPO å‚æ•°
    learning_rate: float = 3e-4
    clip_epsilon: float = 0.2
    value_coef: float = 0.5
    entropy_coef: float = 0.01
    gamma: float = 0.99
    gae_lambda: float = 0.95
    n_epochs: int = 4
    batch_size: int = 4096
    max_grad_norm: float = 0.5

    # è®­ç»ƒé…ç½®
    n_games: int = 16384
    n_hands_per_update: int = 2000
    total_hands: int = 10_000_000

    # æ—¥å¿—å’Œæ£€æŸ¥ç‚¹
    log_interval: int = 1000
    save_interval: int = 10000
    checkpoint_dir: str = "checkpoints"
    wandb_project: str = "6max-poker"
```

---

## 8. è¿è¡Œç¯å¢ƒ

### 8.1 ä¾èµ–

```toml
# pyproject.toml æ–°å¢ä¾èµ–
[project.optional-dependencies]
training = [
    "wandb>=0.15.0",
    "optuna>=3.0.0",  # Phase 2: è¶…å‚æ•°ä¼˜åŒ–
]
```

### 8.2 SLURM æäº¤è„šæœ¬

```bash
#!/bin/bash
#SBATCH --job-name=6max-train
#SBATCH --output=logs/%j.out
#SBATCH --error=logs/%j.err
#SBATCH --gres=gpu:1
#SBATCH --cpus-per-task=4
#SBATCH --mem=32G
#SBATCH --time=24:00:00

# æ¿€æ´»ç¯å¢ƒ
source /path/to/venv/bin/activate

# è®¾ç½® GPU å¯è§æ€§ (æ ¹æ®åˆ†é…è‡ªåŠ¨è®¾ç½®)
export CUDA_VISIBLE_DEVICES=$SLURM_JOB_GPUS

# wandb ç¦»çº¿æ¨¡å¼ (å¯é€‰)
# export WANDB_MODE=offline

# å¯åŠ¨è®­ç»ƒ
python scripts/train.py \
    --config configs/phase1.yaml \
    --wandb-project 6max-poker \
    --wandb-name "phase1-run-${SLURM_JOB_ID}"
```

### 8.3 è®­ç»ƒå¯åŠ¨

```bash
# æœ¬åœ°æµ‹è¯•
python scripts/train.py --config configs/phase1.yaml

# SLURM æäº¤
sbatch scripts/slurm_train.sh

# æŸ¥çœ‹ wandb é¢æ¿
# https://wandb.ai/<username>/6max-poker
```

---

## 9. å®ç°æ¸…å•

| ç»„ä»¶ | ä¼˜å…ˆçº§ | çŠ¶æ€ |
|------|--------|------|
| TrainingConfig | P0 | ğŸ”² å¾…å®ç° |
| RolloutBuffer | P0 | ğŸ”² å¾…å®ç° |
| PPO æŸå¤±è®¡ç®— | P0 | ğŸ”² å¾…å®ç° |
| GAE è®¡ç®— | P0 | ğŸ”² å¾…å®ç° |
| è‡ªåšå¼ˆæ”¶é›† | P0 | ğŸ”² å¾…å®ç° |
| PPOTrainer | P0 | ğŸ”² å¾…å®ç° |
| wandb é›†æˆ | P0 | ğŸ”² å¾…å®ç° |
| æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½ | P0 | ğŸ”² å¾…å®ç° |
| train.py å…¥å£ | P0 | ğŸ”² å¾…å®ç° |
| slurm_train.sh | P0 | ğŸ”² å¾…å®ç° |
| å•å…ƒæµ‹è¯• | P0 | ğŸ”² å¾…å®ç° |
| å†å²å¯¹æ‰‹æ±  | P1 | ğŸ”² åç»­ |
| å¼‚æ­¥ Actor-Learner | P1 | ğŸ”² åç»­ |
| Optuna è¶…å‚æ•°ä¼˜åŒ– | P1 | ğŸ”² åç»­ |
| TD(Î») å¥–åŠ± | P2 | ğŸ”² åç»­ |

---

## é™„å½•

### A. wandb æ—¥å¿—æŒ‡æ ‡

| æŒ‡æ ‡ | è¯´æ˜ |
|------|------|
| train/policy_loss | ç­–ç•¥æŸå¤± |
| train/value_loss | ä»·å€¼æŸå¤± |
| train/entropy | ç­–ç•¥ç†µ |
| train/clip_fraction | PPO è£å‰ªæ¯”ä¾‹ |
| train/learning_rate | å½“å‰å­¦ä¹ ç‡ |
| rollout/hands | å·²æ”¶é›†æ‰‹æ•° |
| rollout/reward_mean | å¹³å‡æ¯æ‰‹å¥–åŠ± |
| rollout/reward_std | å¥–åŠ±æ ‡å‡†å·® |
| eval/win_rate | è¯„ä¼°èƒœç‡ |
| eval/bb_per_100 | BB/100 æ‰‹ |

### B. æ£€æŸ¥ç‚¹æ ¼å¼

```python
checkpoint = {
    'step': current_step,
    'hands': total_hands_played,
    'network_state_dict': network.state_dict(),
    'optimizer_state_dict': optimizer.state_dict(),
    'config': config.__dict__,
    'wandb_run_id': wandb.run.id,
}
torch.save(checkpoint, path)
```

### C. å‚è€ƒèµ„æ–™

- [PPO Paper (Schulman et al.)](https://arxiv.org/abs/1707.06347)
- [GAE Paper](https://arxiv.org/abs/1506.02438)
- [Stable-Baselines3 PPO](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html)
- [CleanRL PPO](https://docs.cleanrl.dev/rl-algorithms/ppo/)
